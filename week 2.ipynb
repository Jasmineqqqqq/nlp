{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4be4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COUNT VECTORIZATION\n",
    "#words into characters\n",
    "#bag of words only frequency atter, does not take into account gramamtical rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b439b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#N-GRAM if n=1 it is uni gram, n=2 bi gram, if n=3 tri gram...3 words together\n",
    "#it will keep context intact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f9ca299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words: ['an', 'bag', 'example', 'is', 'of', 'this', 'words']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Office\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#skipgrams\n",
    "#they skip on eor two words depending\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "#single doc(','-separated ech doc)\n",
    "string=['This is an example of bag of words']\n",
    "#this step will convert tp tokens\n",
    "vect1=CountVectorizer()\n",
    "vect1.fit_transform(string)\n",
    "print('bag of words:', vect1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4bdbe04e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 5, 'is': 3, 'an': 0, 'example': 2, 'of': 4, 'bag': 1, 'words': 6}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect1.vocabulary_ #this will also give index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0e7c6ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_vect=CountVectorizer()\n",
    "c_vect.fit(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ef536b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Present at  [[0 0 0 2 1 0 0]]\n",
      "original indexes ['an', 'bag', 'example', 'is', 'of', 'this', 'words']\n"
     ]
    }
   ],
   "source": [
    "string2=['Lets understand is of word is']\n",
    "c_new_vect=c_vect.transform(string2)\n",
    "print(\"Text Present at \",c_new_vect.toarray())\n",
    "#compare with indexes\n",
    "print(\"original indexes\", vect1.get_feature_names()) #is is occuring 2 times, hence 2 is mentioned frok original sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c14c7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
      "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
      "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
      "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
      "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
      "                            'itself', ...])\n"
     ]
    }
   ],
   "source": [
    "#bag if wrdsusing stopwrords.u can avoide separately keping stop words\n",
    "stpwords=stopwords.words('english')\n",
    "string=[\"Thiis an example of bag of words\"]\n",
    "vect1=CountVectorizer(stop_words=stpwords)\n",
    "print(vect1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6b1220db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words: ['bag', 'example', 'thiis', 'words']\n",
      "vocab: {'thiis': 2, 'example': 1, 'bag': 0, 'words': 3}\n"
     ]
    }
   ],
   "source": [
    "vect1.fit_transform(string)\n",
    "print(\"bag of words:\", vect1.get_feature_names())\n",
    "print(\"vocab:\", vect1.vocabulary_) #stoppwrds from string is removed, ony imp words seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c439a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using function\n",
    "def text_matrix(message, countvect):\n",
    "    terms_doc=countvect.fit_transform(message)\n",
    "    return pd.DataFrame(terms_doc.toarray(),columns=countvect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e8beeb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "below matrix is a bag of words\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>bet</th>\n",
       "      <th>but</th>\n",
       "      <th>for</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>language</th>\n",
       "      <th>making</th>\n",
       "      <th>matra</th>\n",
       "      <th>natural</th>\n",
       "      <th>only</th>\n",
       "      <th>practice</th>\n",
       "      <th>processing</th>\n",
       "      <th>progress</th>\n",
       "      <th>slowly</th>\n",
       "      <th>success</th>\n",
       "      <th>the</th>\n",
       "      <th>there</th>\n",
       "      <th>we</th>\n",
       "      <th>will</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   are  bet  but  for  in  is  language  making  matra  natural  only  \\\n",
       "0    1    0    0    0   1   0         1       1      0        1     0   \n",
       "1    0    1    1    1   0   1         0       0      1        0     1   \n",
       "\n",
       "   practice  processing  progress  slowly  success  the  there  we  will  \n",
       "0         0           1         1       2        0    0      0   1     0  \n",
       "1         1           0         0       0        1    1      1   1     1  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message=['We are slowly slowly making progress in Natural language Processing', \n",
    "         'We will bet there\", \"but practice is the only matra for success']\n",
    "c_vect=CountVectorizer()\n",
    "print(\"below matrix is a bag of words\")\n",
    "text_matrix(message, c_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eb338182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram: ['an', 'example', 'gram', 'is', 'of', 'this']\n",
      "2-gram: ['an example', 'example of', 'is an', 'of gram', 'this is']\n",
      "3-gram: ['an example of', 'example of gram', 'is an example', 'this is an']\n",
      "4-gram: ['an example of gram', 'is an example of', 'this is an example']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Office\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "string=['This is an example of a gram']\n",
    "vect1=CountVectorizer(ngram_range=(1,1))\n",
    "vect1.fit_transform(string)\n",
    "vect2=CountVectorizer(ngram_range=(2,2))\n",
    "vect2.fit_transform(string)\n",
    "vect3=CountVectorizer(ngram_range=(3,3))\n",
    "vect3.fit_transform(string)\n",
    "vect4=CountVectorizer(ngram_range=(4,4))\n",
    "vect4.fit_transform(string)\n",
    "print(\"1-gram:\", vect1.get_feature_names())\n",
    "print(\"2-gram:\", vect2.get_feature_names())\n",
    "print(\"3-gram:\", vect3.get_feature_names())\n",
    "print(\"4-gram:\", vect4.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c3944eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>be</th>\n",
       "      <th>can</th>\n",
       "      <th>confusing</th>\n",
       "      <th>example</th>\n",
       "      <th>how</th>\n",
       "      <th>idf</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>see</th>\n",
       "      <th>this</th>\n",
       "      <th>we</th>\n",
       "      <th>wll</th>\n",
       "      <th>works</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         an   be  can  confusing   example       how  idf        is        it  \\\n",
       "0  0.316228  0.0  0.0        0.0  0.316228  0.316228  0.0  0.316228  0.316228   \n",
       "1  0.000000  0.5  0.5        0.5  0.000000  0.000000  0.5  0.000000  0.000000   \n",
       "\n",
       "        see      this        we       wll     works  \n",
       "0  0.316228  0.316228  0.316228  0.316228  0.316228  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "tfid=TfidfVectorizer(smooth_idf=False)\n",
    "doc=['this is an example.' 'We wll see how it works.', 'IDF can be confusing']\n",
    "doc_vector=tfid.fit_transform(doc)\n",
    "#print(tfid.get_feature_names())\n",
    "df=pd.DataFrame(doc_vector.todense(), columns=tfid.get_feature_names())\n",
    "df #print doc_vetor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "604ef812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_matrix(message,countvect):\n",
    "    terms_doc=countvect.fit_transform(message)\n",
    "    return pd.DataFrame(terms_doc.toarray(),columns=countvect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6c263224",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will call the fucntion created earlier\n",
    "feb_message=[\"What is that covid covid\", \"covid is nothing\", \"covid cases are dropping\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e7357bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Office\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>cases</th>\n",
       "      <th>covid</th>\n",
       "      <th>dropping</th>\n",
       "      <th>is</th>\n",
       "      <th>nothing</th>\n",
       "      <th>that</th>\n",
       "      <th>what</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.592567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501651</td>\n",
       "      <td>0.501651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.322745</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        are     cases     covid  dropping        is   nothing      that  \\\n",
       "0  0.000000  0.000000  0.592567  0.000000  0.381519  0.000000  0.501651   \n",
       "1  0.000000  0.000000  0.425441  0.000000  0.547832  0.720333  0.000000   \n",
       "2  0.546454  0.546454  0.322745  0.546454  0.000000  0.000000  0.000000   \n",
       "\n",
       "       what  \n",
       "0  0.501651  \n",
       "1  0.000000  \n",
       "2  0.000000  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf=TfidfVectorizer()\n",
    "#passign same message\n",
    "text_matrix(feb_message, tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f8d4a5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Office\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>covid</th>\n",
       "      <th>is</th>\n",
       "      <th>what</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.757264</td>\n",
       "      <td>0.378632</td>\n",
       "      <td>0.532154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.704909</td>\n",
       "      <td>0.501549</td>\n",
       "      <td>0.501549</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bad     covid        is      what\n",
       "0  0.000000  0.757264  0.378632  0.532154\n",
       "1  0.704909  0.501549  0.501549  0.000000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importanceof covid increased based on occurance and total doc\n",
    "jul_message=[\"what is covid covid\",\n",
    "            \"covid is bad\"]\n",
    "text_matrix(jul_message,tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8329023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "arr=['Car was cleaned by jack',\n",
    "    'Jack was cleaned by the car']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "29ea8860",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=['Car was cleaned by jack',\n",
    "    'Jack was cleaned by the car']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0a08b9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names\n",
      " ['by jack', 'by the', 'car was', 'cleaned by', 'jack was', 'the car', 'was cleaned']\n",
      "Array\n",
      " [[1 0 1 1 0 0 1]\n",
      " [0 1 0 1 1 1 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Office\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#if u wnat o take into account frequencies\n",
    "vectorizer=CountVectorizer(ngram_range=(2,2))\n",
    "#the ngram range specifies the configuration\n",
    "X=vectorizer.fit_transform(arr)\n",
    "#testing ngram\n",
    "print('Feature Names\\n', vectorizer.get_feature_names())\n",
    "print('Array\\n', X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e3af5e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57615236 0.         0.57615236 0.40993715 0.         0.\n",
      "  0.40993715]\n",
      " [0.         0.49922133 0.         0.35520009 0.49922133 0.49922133\n",
      "  0.35520009]]\n"
     ]
    }
   ],
   "source": [
    "#tdif vectorizer testing\n",
    "#u can still specify ngrams here\n",
    "vectorizer=TfidfVectorizer(ngram_range=(2,2))\n",
    "X=vectorizer.fit_transform(arr)\n",
    "#testig tdif values +ingrams\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "58364c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.40546511 0.         1.40546511 1.         0.         0.\n",
      "  1.        ]\n",
      " [0.         1.40546511 0.         1.         1.40546511 1.40546511\n",
      "  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#testing tdif wthout normalization.u can specify grams here\n",
    "vectorizer=TfidfVectorizer(ngram_range=(2,2), norm=None)\n",
    "X=vectorizer.fit_transform(arr)\n",
    "#testing TDIF before normalization\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d18eb624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7067ba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"he Ministry of Home Affairs (MHA) has cancelled the Foreign Contribution Regulation Act (FCRA) licence of the Rajiv Gandhi Foundation (RGF) and Rajiv Gandhi Charitable Trust (RGCT), organisations associated with the Gandhi family, for alleged violations of the law.\n",
    "\n",
    "In 2020, the central government had set up an inter-ministerial committee to “coordinate investigations” into the funding of three trusts linked to the Nehru-Gandhi family — the RGF, RGCT and the Indira Gandhi Memorial Trust.\n",
    "\n",
    "The BJP had alleged at the time that the RGF had received funds from China to “conduct studies that are not in the interest of the country”. Alongside, investigations into violation of various legal provisions of PMLA (Prevention of Money Laundering Act), Income Tax Act, FCRA etc., were to be conducted as well.\n",
    "All three organisations are headed by Sonia Gandhi. Former Prime Minister Manmohan Singh and Congress leaders P Chidambaram, Rahul Gandhi, and Priyanka Gandhi Vadra are among the trustees at the RGF.\n",
    "\n",
    "Here is what the case is about, and what the cancellation of licenses can mean.\n",
    "\n",
    "What are the allegations against the RGF?\n",
    "In June 2020, after the Congress party criticised the government over its handling of border issues with China in Ladakh, the BJP had said that the Congress had no moral right to talk about the security of the country after having accepted money from China.\n",
    "\n",
    "“Today I was shocked to watch on TV that in 2005-06 People’s Republic of China and the Chinese embassy gave a fat sum to RGF. This is a secret relationship between Congress and China. These people take funds from China and then conduct studies that are not in the interest of the country. These studies create the environment for that. The nation wants to know for what they were paid and what study they conducted,” BJP president J P Nadda had said.\n",
    "\n",
    "“You take $300,000 donation and teach us nationalism,” Nadda said.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "851344c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he ministry of home affairs mha has cancelled the foreign contribution regulation act fcra licence of the rajiv gandhi foundation rgf and rajiv gandhi charitable trust rgct organisations associated with the gandhi family for alleged violations of the law in the central government had set up an interministerial committee to “coordinate investigations” into the funding of three trusts linked to the nehrugandhi family — the rgf rgct and the indira gandhi memorial trust the bjp had alleged at the time that the rgf had received funds from china to “conduct studies that are not in the interest of the country” alongside investigations into violation of various legal provisions of pmla prevention of money laundering act income tax act fcra etc were to be conducted as well all three organisations are headed by sonia gandhi former prime minister manmohan singh and congress leaders p chidambaram rahul gandhi and priyanka gandhi vadra are among the trustees at the rgf here is what the case is about and what the cancellation of licenses can mean what are the allegations against the rgf in june after the congress party criticised the government over its handling of border issues with china in ladakh the bjp had said that the congress had no moral right to talk about the security of the country after having accepted money from china “today i was shocked to watch on tv that in people’s republic of china and the chinese embassy gave a fat sum to rgf this is a secret relationship between congress and china these people take funds from china and then conduct studies that are not in the interest of the country these studies create the environment for that the nation wants to know for what they were paid and what study they conducted” bjp president j p nadda had said “you take donation and teach us nationalism” nadda said\n"
     ]
    }
   ],
   "source": [
    "#it will replace punctuation of comma with sace\n",
    "paragraph=paragraph.translate(str.maketrans('','',string.punctuation))\n",
    "text=re.sub(r'\\[[0-9]*\\]',' ', paragraph)\n",
    "text=re.sub(r'\\s+',' ', text)\n",
    "text=text.lower()\n",
    "text=re.sub(r'\\d',' ', text)\n",
    "text=re.sub(r'\\s+',' ', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fd700c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he ministry of home affairs mha has cancelled the foreign contribution regulation act fcra licence of the rajiv gandhi foundation rgf and rajiv gandhi charitable trust rgct organisations associated with the gandhi family for alleged violations of the law in the central government had set up an interministerial committee to “coordinate investigations” into the funding of three trusts linked to the nehrugandhi family — the rgf rgct and the indira gandhi memorial trust the bjp had alleged at the time that the rgf had received funds from china to “conduct studies that are not in the interest of the country” alongside investigations into violation of various legal provisions of pmla prevention of money laundering act income tax act fcra etc were to be conducted as well all three organisations are headed by sonia gandhi former prime minister manmohan singh and congress leaders p chidambaram rahul gandhi and priyanka gandhi vadra are among the trustees at the rgf here is what the case is about and what the cancellation of licenses can mean what are the allegations against the rgf in june after the congress party criticised the government over its handling of border issues with china in ladakh the bjp had said that the congress had no moral right to talk about the security of the country after having accepted money from china “today i was shocked to watch on tv that in people’s republic of china and the chinese embassy gave a fat sum to rgf this is a secret relationship between congress and china these people take funds from china and then conduct studies that are not in the interest of the country these studies create the environment for that the nation wants to know for what they were paid and what study they conducted” bjp president j p nadda had said “you take donation and teach us nationalism” nadda said']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Office\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#preparing the dataset\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "sentences=nltk.sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "84c4b639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'ministry', 'of', 'home', 'affairs', 'mha', 'has', 'cancelled', 'the', 'foreign', 'contribution', 'regulation', 'act', 'fcra', 'licence', 'of', 'the', 'rajiv', 'gandhi', 'foundation', 'rgf', 'and', 'rajiv', 'gandhi', 'charitable', 'trust', 'rgct', 'organisations', 'associated', 'with', 'the', 'gandhi', 'family', 'for', 'alleged', 'violations', 'of', 'the', 'law', 'in', 'the', 'central', 'government', 'had', 'set', 'up', 'an', 'interministerial', 'committee', 'to', '“', 'coordinate', 'investigations', '”', 'into', 'the', 'funding', 'of', 'three', 'trusts', 'linked', 'to', 'the', 'nehrugandhi', 'family', '—', 'the', 'rgf', 'rgct', 'and', 'the', 'indira', 'gandhi', 'memorial', 'trust', 'the', 'bjp', 'had', 'alleged', 'at', 'the', 'time', 'that', 'the', 'rgf', 'had', 'received', 'funds', 'from', 'china', 'to', '“', 'conduct', 'studies', 'that', 'are', 'not', 'in', 'the', 'interest', 'of', 'the', 'country', '”', 'alongside', 'investigations', 'into', 'violation', 'of', 'various', 'legal', 'provisions', 'of', 'pmla', 'prevention', 'of', 'money', 'laundering', 'act', 'income', 'tax', 'act', 'fcra', 'etc', 'were', 'to', 'be', 'conducted', 'as', 'well', 'all', 'three', 'organisations', 'are', 'headed', 'by', 'sonia', 'gandhi', 'former', 'prime', 'minister', 'manmohan', 'singh', 'and', 'congress', 'leaders', 'p', 'chidambaram', 'rahul', 'gandhi', 'and', 'priyanka', 'gandhi', 'vadra', 'are', 'among', 'the', 'trustees', 'at', 'the', 'rgf', 'here', 'is', 'what', 'the', 'case', 'is', 'about', 'and', 'what', 'the', 'cancellation', 'of', 'licenses', 'can', 'mean', 'what', 'are', 'the', 'allegations', 'against', 'the', 'rgf', 'in', 'june', 'after', 'the', 'congress', 'party', 'criticised', 'the', 'government', 'over', 'its', 'handling', 'of', 'border', 'issues', 'with', 'china', 'in', 'ladakh', 'the', 'bjp', 'had', 'said', 'that', 'the', 'congress', 'had', 'no', 'moral', 'right', 'to', 'talk', 'about', 'the', 'security', 'of', 'the', 'country', 'after', 'having', 'accepted', 'money', 'from', 'china', '“', 'today', 'i', 'was', 'shocked', 'to', 'watch', 'on', 'tv', 'that', 'in', 'people', '’', 's', 'republic', 'of', 'china', 'and', 'the', 'chinese', 'embassy', 'gave', 'a', 'fat', 'sum', 'to', 'rgf', 'this', 'is', 'a', 'secret', 'relationship', 'between', 'congress', 'and', 'china', 'these', 'people', 'take', 'funds', 'from', 'china', 'and', 'then', 'conduct', 'studies', 'that', 'are', 'not', 'in', 'the', 'interest', 'of', 'the', 'country', 'these', 'studies', 'create', 'the', 'environment', 'for', 'that', 'the', 'nation', 'wants', 'to', 'know', 'for', 'what', 'they', 'were', 'paid', 'and', 'what', 'study', 'they', 'conducted', '”', 'bjp', 'president', 'j', 'p', 'nadda', 'had', 'said', '“', 'you', 'take', 'donation', 'and', 'teach', 'us', 'nationalism', '”', 'nadda', 'said']]\n"
     ]
    }
   ],
   "source": [
    "sent_word=[nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "print(sent_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c094b7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc=string.punctuation\n",
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4bbd9f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Office\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#sent_word=[nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "for i in range(len(sent_word)):\n",
    "    sent_word[i]=[word for word in sent_word[i]\n",
    "                  if word not in stopwords.words('english') if word not in punc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "023260c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Word2Vec\n",
      "  Using cached word2vec-0.11.1.tar.gz (42 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: joblib in c:\\users\\office\\anaconda3\\lib\\site-packages (from Word2Vec) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.2 in c:\\users\\office\\anaconda3\\lib\\site-packages (from Word2Vec) (1.21.5)\n",
      "Building wheels for collected packages: Word2Vec\n",
      "  Building wheel for Word2Vec (pyproject.toml): started\n",
      "  Building wheel for Word2Vec (pyproject.toml): finished with status 'error'\n",
      "Failed to build Word2Vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for Word2Vec (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [99 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib\n",
      "  creating build\\lib\\word2vec\n",
      "  copying word2vec\\io.py -> build\\lib\\word2vec\n",
      "  copying word2vec\\scripts_interface.py -> build\\lib\\word2vec\n",
      "  copying word2vec\\utils.py -> build\\lib\\word2vec\n",
      "  copying word2vec\\wordclusters.py -> build\\lib\\word2vec\n",
      "  copying word2vec\\wordvectors.py -> build\\lib\\word2vec\n",
      "  copying word2vec\\_generated_version.py -> build\\lib\\word2vec\n",
      "  copying word2vec\\__init__.py -> build\\lib\\word2vec\n",
      "  creating build\\lib\\word2vec\\tests\n",
      "  copying word2vec\\tests\\test_core.py -> build\\lib\\word2vec\\tests\n",
      "  copying word2vec\\tests\\test_import.py -> build\\lib\\word2vec\\tests\n",
      "  copying word2vec\\tests\\test_scripts_present.py -> build\\lib\\word2vec\\tests\n",
      "  copying word2vec\\tests\\__init__.py -> build\\lib\\word2vec\\tests\n",
      "  running egg_info\n",
      "  writing word2vec.egg-info\\PKG-INFO\n",
      "  writing dependency_links to word2vec.egg-info\\dependency_links.txt\n",
      "  writing requirements to word2vec.egg-info\\requires.txt\n",
      "  writing top-level names to word2vec.egg-info\\top_level.txt\n",
      "  reading manifest file 'word2vec.egg-info\\SOURCES.txt'\n",
      "  adding license file 'LICENSE.txt'\n",
      "  writing manifest file 'word2vec.egg-info\\SOURCES.txt'\n",
      "  C:\\Users\\Office\\AppData\\Local\\Temp\\pip-build-env-qea7ymm6\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'word2vec.includes' as data is deprecated, please list it in `packages`.\n",
      "      !!\n",
      "  \n",
      "  \n",
      "      ############################\n",
      "      # Package would be ignored #\n",
      "      ############################\n",
      "      Python recognizes 'word2vec.includes' as an importable package,\n",
      "      but it is not listed in the `packages` configuration of setuptools.\n",
      "  \n",
      "      'word2vec.includes' has been automatically added to the distribution only\n",
      "      because it may contain data files, but this behavior is likely to change\n",
      "      in future versions of setuptools (and therefore is considered deprecated).\n",
      "  \n",
      "      Please make sure that 'word2vec.includes' is included as a package by using\n",
      "      the `packages` configuration field or the proper discovery methods\n",
      "      (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "      instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "      You can read more about \"package discovery\" and \"data files\" on setuptools\n",
      "      documentation page.\n",
      "  \n",
      "  \n",
      "  !!\n",
      "  \n",
      "    check.warn(importable)\n",
      "  C:\\Users\\Office\\AppData\\Local\\Temp\\pip-build-env-qea7ymm6\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'word2vec.includes.win32' as data is deprecated, please list it in `packages`.\n",
      "      !!\n",
      "  \n",
      "  \n",
      "      ############################\n",
      "      # Package would be ignored #\n",
      "      ############################\n",
      "      Python recognizes 'word2vec.includes.win32' as an importable package,\n",
      "      but it is not listed in the `packages` configuration of setuptools.\n",
      "  \n",
      "      'word2vec.includes.win32' has been automatically added to the distribution only\n",
      "      because it may contain data files, but this behavior is likely to change\n",
      "      in future versions of setuptools (and therefore is considered deprecated).\n",
      "  \n",
      "      Please make sure that 'word2vec.includes.win32' is included as a package by using\n",
      "      the `packages` configuration field or the proper discovery methods\n",
      "      (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "      instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "      You can read more about \"package discovery\" and \"data files\" on setuptools\n",
      "      documentation page.\n",
      "  \n",
      "  \n",
      "  !!\n",
      "  \n",
      "    check.warn(importable)\n",
      "  creating build\\lib\\word2vec\\includes\n",
      "  copying word2vec\\includes\\Makefile -> build\\lib\\word2vec\\includes\n",
      "  copying word2vec\\includes\\compute-accuracy.c -> build\\lib\\word2vec\\includes\n",
      "  copying word2vec\\includes\\distance.c -> build\\lib\\word2vec\\includes\n",
      "  copying word2vec\\includes\\word-analogy.c -> build\\lib\\word2vec\\includes\n",
      "  copying word2vec\\includes\\word2phrase.c -> build\\lib\\word2vec\\includes\n",
      "  copying word2vec\\includes\\word2vec-sentence2vec.c -> build\\lib\\word2vec\\includes\n",
      "  copying word2vec\\includes\\word2vec.c -> build\\lib\\word2vec\\includes\n",
      "  creating build\\lib\\word2vec\\includes\\win32\n",
      "  copying word2vec\\includes\\win32\\Makefile -> build\\lib\\word2vec\\includes\\win32\n",
      "  copying word2vec\\includes\\win32\\compute-accuracy.c -> build\\lib\\word2vec\\includes\\win32\n",
      "  copying word2vec\\includes\\win32\\distance.c -> build\\lib\\word2vec\\includes\\win32\n",
      "  copying word2vec\\includes\\win32\\win32-port.h -> build\\lib\\word2vec\\includes\\win32\n",
      "  copying word2vec\\includes\\win32\\word-analogy.c -> build\\lib\\word2vec\\includes\\win32\n",
      "  copying word2vec\\includes\\win32\\word2phrase.c -> build\\lib\\word2vec\\includes\\win32\n",
      "  copying word2vec\\includes\\win32\\word2vec.c -> build\\lib\\word2vec\\includes\\win32\n",
      "  installing to build\\bdist.win-amd64\\wheel\n",
      "  running install\n",
      "  Running custom Install command\n",
      "  Compiling: gcc C:\\Users\\Office\\AppData\\Local\\Temp\\pip-install-f_69ghmm\\word2vec_6f1fb810a25d4e30872da0259203b193\\word2vec\\includes\\win32/word2vec.c -o Scripts\\word2vec.exe -O2 -Wall -funroll-loops\n",
      "  error: [WinError 2] The system cannot find the file specified\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for Word2Vec\n",
      "ERROR: Could not build wheels for Word2Vec, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "#train the word2vec model\n",
    "!pip install Word2Vec\n",
    "model=Word2Vec(sent_word, min_count=1)\n",
    "words=list(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9e2ba861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gandhi',\n",
       " 'china',\n",
       " 'rgf',\n",
       " '“',\n",
       " '”',\n",
       " 'congress',\n",
       " 'country',\n",
       " 'studies',\n",
       " 'act',\n",
       " 'said',\n",
       " 'bjp',\n",
       " 'conduct',\n",
       " 'funds',\n",
       " 'government',\n",
       " 'p',\n",
       " 'interest',\n",
       " 'investigations',\n",
       " 'alleged',\n",
       " 'three',\n",
       " 'people',\n",
       " 'family',\n",
       " 'organisations',\n",
       " 'trust',\n",
       " 'take',\n",
       " 'rajiv',\n",
       " 'money',\n",
       " 'fcra',\n",
       " 'conducted',\n",
       " 'nadda',\n",
       " 'rgct',\n",
       " '—',\n",
       " 'time',\n",
       " 'memorial',\n",
       " 'prevention',\n",
       " 'provisions',\n",
       " 'pmla',\n",
       " 'received',\n",
       " 'legal',\n",
       " 'various',\n",
       " 'violation',\n",
       " 'alongside',\n",
       " 'indira',\n",
       " 'nationalism',\n",
       " 'nehrugandhi',\n",
       " 'linked',\n",
       " 'home',\n",
       " 'affairs',\n",
       " 'mha',\n",
       " 'cancelled',\n",
       " 'foreign',\n",
       " 'contribution',\n",
       " 'regulation',\n",
       " 'licence',\n",
       " 'foundation',\n",
       " 'charitable',\n",
       " 'associated',\n",
       " 'violations',\n",
       " 'law',\n",
       " 'central',\n",
       " 'set',\n",
       " 'interministerial',\n",
       " 'committee',\n",
       " 'income',\n",
       " 'coordinate',\n",
       " 'funding',\n",
       " 'trusts',\n",
       " 'laundering',\n",
       " 'well',\n",
       " 'tax',\n",
       " 'sum',\n",
       " 'security',\n",
       " 'accepted',\n",
       " 'today',\n",
       " 'shocked',\n",
       " 'watch',\n",
       " 'tv',\n",
       " '’',\n",
       " 'republic',\n",
       " 'chinese',\n",
       " 'embassy',\n",
       " 'gave',\n",
       " 'fat',\n",
       " 'secret',\n",
       " 'right',\n",
       " 'relationship',\n",
       " 'create',\n",
       " 'environment',\n",
       " 'nation',\n",
       " 'wants',\n",
       " 'know',\n",
       " 'paid',\n",
       " 'study',\n",
       " 'president',\n",
       " 'j',\n",
       " 'donation',\n",
       " 'teach',\n",
       " 'talk',\n",
       " 'moral',\n",
       " 'etc',\n",
       " 'vadra',\n",
       " 'us',\n",
       " 'headed',\n",
       " 'sonia',\n",
       " 'former',\n",
       " 'prime',\n",
       " 'minister',\n",
       " 'manmohan',\n",
       " 'singh',\n",
       " 'leaders',\n",
       " 'chidambaram',\n",
       " 'rahul',\n",
       " 'priyanka',\n",
       " 'among',\n",
       " 'ladakh',\n",
       " 'trustees',\n",
       " 'case',\n",
       " 'cancellation',\n",
       " 'licenses',\n",
       " 'mean',\n",
       " 'allegations',\n",
       " 'june',\n",
       " 'party',\n",
       " 'criticised',\n",
       " 'handling',\n",
       " 'border',\n",
       " 'issues',\n",
       " 'ministry']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8be02038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.0471590e-04  3.0788372e-04  5.0821579e-03  8.9895129e-03\n",
      " -9.2515098e-03 -7.1943933e-03  6.4970446e-03  9.0687443e-03\n",
      " -5.0667673e-03 -3.8128749e-03  7.3199673e-03 -1.6508432e-03\n",
      " -4.5349356e-03  6.5836054e-03 -4.8475517e-03 -1.8440796e-03\n",
      "  2.9189307e-03  9.3511603e-04 -8.2779005e-03 -9.5828483e-03\n",
      "  7.3426915e-03  5.0853104e-03  6.7652608e-03  6.9353153e-04\n",
      "  6.4338800e-03 -3.3589627e-03 -1.0121141e-03  5.7481076e-03\n",
      " -7.5601679e-03 -3.9746291e-03 -7.4464134e-03 -8.8292739e-04\n",
      "  9.5428573e-03 -7.4162027e-03 -2.3455559e-03 -1.8005063e-03\n",
      "  8.0898199e-03 -5.9565962e-03  2.8722716e-05 -4.8336987e-03\n",
      " -9.5563447e-03  4.9421857e-03 -8.7844515e-03 -4.4438289e-03\n",
      "  2.1733626e-05 -3.2302403e-04 -7.7289240e-03  9.5695984e-03\n",
      "  5.0165737e-03  9.2501296e-03 -8.1182560e-03  4.4339243e-03\n",
      " -4.1773855e-03  8.9204329e-04  8.4581440e-03 -4.4153370e-03\n",
      "  4.5277639e-03 -6.7552472e-03 -3.6325371e-03  9.4145313e-03\n",
      " -1.5470723e-03  3.1377038e-04 -4.0784334e-03 -7.6326570e-03\n",
      " -1.5355792e-03  2.5385201e-03 -8.7784464e-04  5.6124977e-03\n",
      " -2.7958665e-03  2.3687058e-03  5.4514785e-03  8.4652230e-03\n",
      " -1.4131000e-03 -9.1696074e-03  4.4607427e-03  5.5313529e-04\n",
      "  7.4381200e-03 -7.8497932e-04 -2.6199035e-03 -8.7112887e-03\n",
      " -9.4035693e-04  2.7932539e-03  5.3244042e-03  7.0970478e-03\n",
      " -5.7980292e-03  1.8505672e-03  6.2311231e-03 -4.7431998e-03\n",
      " -3.0534095e-03  6.7838347e-03  1.6626314e-03  2.9974882e-04\n",
      "  3.4889001e-03  2.2706104e-04  9.6999230e-03  5.1244339e-03\n",
      " -8.8883024e-03 -7.0530479e-03  9.0584054e-04  6.4409287e-03]\n"
     ]
    }
   ],
   "source": [
    "#test thr word vector\n",
    "vector=model.wv['gandhi']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "74fae5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('today', 0.2915095388889313),\n",
       " ('talk', 0.23278309404850006),\n",
       " ('investigations', 0.22225676476955414),\n",
       " ('said', 0.21703043580055237),\n",
       " ('trusts', 0.20610640943050385)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mst similar wwords\n",
    "similar=model.wv.most_similar('gandhi', topn=5)\n",
    "similar #u can seen percentage of similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "921ef107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shocked', 0.3086913824081421),\n",
       " ('tax', 0.28038954734802246),\n",
       " ('chinese', 0.23621989786624908),\n",
       " ('wants', 0.20300668478012085),\n",
       " ('committee', 0.19181187450885773)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mst similar wwords\n",
    "similar=model.wv.most_similar('congress', topn=5)\n",
    "similar #u can seen percentage of similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "34e7ed5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05644265"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can also find most similar words\n",
    "model.wv.similarity(w1='congress', w2='gandhi') #accuracy also depends o the vastness of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c96833fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.046035375"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can also find most similar words\n",
    "model.wv.similarity(w1='congress', w2='government')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2f9314a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gandhi'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter on non similarity\n",
    "model.wv.doesnt_match(['government', 'congress', 'gandhi']) #shown the word that does not match "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0a8abe3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordvecs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8584\\1595856499.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtsne_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTSNE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pca'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mcoordinates\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtsne_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordvecs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wordvecs' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "vocab=['government', 'congress', 'gandhi']\n",
    "def tsne_plot(model):\n",
    "    labels=[]\n",
    "    wordvecs=[]\n",
    "    for word in vocab:\n",
    "        wordvecs.append(model[word])\n",
    "        labels.append(word)\n",
    "        tsne_model=TSNE(perplexity=3, n_components=2, init='pca', random_state=42)\n",
    "coordinates= tsne_model.fit_transform(wordvecs)\n",
    "x=[]\n",
    "y=[]\n",
    "for values in coordinates:\n",
    "    x.append(value[0])\n",
    "    y.append(value[1])\n",
    "    \n",
    "plt.figure(figsize=(10,6))\n",
    "for i in range(len(x)):\n",
    "    plt.scatter(x[i], y[i])\n",
    "    plt.annotate(labels[i],\n",
    "                 xy=(x[i], y[i]),\n",
    "                 xytext=(2,2),\n",
    "                 textcoords='offset points', ha='right')\n",
    "    plt.show()\n",
    "tsne_plot(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5da8b276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import  TSNE\n",
    "vocab = ['government', 'congress', 'gandhi','india']\n",
    "def tsne_plot(model):\n",
    "    labels = []\n",
    "    wordvecs = []\n",
    "\n",
    "    for word in vocab:\n",
    "        wordvecs.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=3, n_components=2, init='pca', random_state=42)\n",
    "    coordinates = tsne_model.fit_transform(wordvecs)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in coordinates:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(10,6)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(2, 2),\n",
    "                     textcoords='offset points', ha='left')\n",
    "\n",
    "    plt.show()\n",
    "    tsne_plot(model)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d7caf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
